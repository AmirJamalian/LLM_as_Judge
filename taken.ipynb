{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "449c0fb2",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-01-11T10:09:02.239454Z",
     "iopub.status.busy": "2025-01-11T10:09:02.239207Z",
     "iopub.status.idle": "2025-01-11T10:09:21.740612Z",
     "shell.execute_reply": "2025-01-11T10:09:21.739676Z"
    },
    "papermill": {
     "duration": 19.505928,
     "end_time": "2025-01-11T10:09:21.742130",
     "exception": false,
     "start_time": "2025-01-11T10:09:02.236202",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Location '/kaggle/input/hf-libraries/transformers' is ignored: it is either a non-existing path or lacks a specific scheme.\u001b[0m\u001b[33m\r\n",
      "\u001b[0mtransformers is installed\n",
      "using gpu is True\n",
      "/kaggle/input/phi-3.5-mini-instruct/pytorch/default/1/model.safetensors.index.json\n",
      "/kaggle/input/phi-3.5-mini-instruct/pytorch/default/1/CODE_OF_CONDUCT.md\n",
      "/kaggle/input/phi-3.5-mini-instruct/pytorch/default/1/config.json\n",
      "/kaggle/input/phi-3.5-mini-instruct/pytorch/default/1/model-00001-of-00002.safetensors\n",
      "/kaggle/input/phi-3.5-mini-instruct/pytorch/default/1/LICENSE\n",
      "/kaggle/input/phi-3.5-mini-instruct/pytorch/default/1/model-00002-of-00002.safetensors\n",
      "/kaggle/input/phi-3.5-mini-instruct/pytorch/default/1/modeling_phi3.py\n",
      "/kaggle/input/phi-3.5-mini-instruct/pytorch/default/1/README.md\n",
      "/kaggle/input/phi-3.5-mini-instruct/pytorch/default/1/SECURITY.md\n",
      "/kaggle/input/phi-3.5-mini-instruct/pytorch/default/1/tokenizer.json\n",
      "/kaggle/input/phi-3.5-mini-instruct/pytorch/default/1/tokenizer_config.json\n",
      "/kaggle/input/phi-3.5-mini-instruct/pytorch/default/1/gitattributes\n",
      "/kaggle/input/phi-3.5-mini-instruct/pytorch/default/1/sample_finetune.py\n",
      "/kaggle/input/phi-3.5-mini-instruct/pytorch/default/1/special_tokens_map.json\n",
      "/kaggle/input/phi-3.5-mini-instruct/pytorch/default/1/tokenizer.model\n",
      "/kaggle/input/phi-3.5-mini-instruct/pytorch/default/1/NOTICE.md\n",
      "/kaggle/input/phi-3.5-mini-instruct/pytorch/default/1/added_tokens.json\n",
      "/kaggle/input/phi-3.5-mini-instruct/pytorch/default/1/configuration_phi3.py\n",
      "/kaggle/input/phi-3.5-mini-instruct/pytorch/default/1/generation_config.json\n",
      "/kaggle/input/llms-you-cant-please-them-all/sample_submission.csv\n",
      "/kaggle/input/llms-you-cant-please-them-all/test.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1097671</td>\n",
       "      <td>Compare and contrast the importance of self-re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1726150</td>\n",
       "      <td>Evaluate the effectiveness of management consu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3211968</td>\n",
       "      <td>Discuss the role of self-reliance in achieving...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                              topic\n",
       "0  1097671  Compare and contrast the importance of self-re...\n",
       "1  1726150  Evaluate the effectiveness of management consu...\n",
       "2  3211968  Discuss the role of self-reliance in achieving..."
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "!pip install -q -U transformers --no-index --find-links /kaggle/input/hf-libraries/transformers\n",
    "print(\"transformers is installed\")\n",
    "\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM, AutoModel\n",
    "import torch\n",
    "import logging\n",
    "logging.getLogger('transformers').setLevel(logging.ERROR)\n",
    "\n",
    "print(f'using gpu is {torch.cuda.is_available()}')\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n",
    "\n",
    "test_df = pd.read_csv(\"/kaggle/input/llms-you-cant-please-them-all/test.csv\")\n",
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6edcbc80",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-11T10:09:21.748048Z",
     "iopub.status.busy": "2025-01-11T10:09:21.747795Z",
     "iopub.status.idle": "2025-01-11T10:10:24.368702Z",
     "shell.execute_reply": "2025-01-11T10:10:24.367966Z"
    },
    "papermill": {
     "duration": 62.625232,
     "end_time": "2025-01-11T10:10:24.370130",
     "exception": false,
     "start_time": "2025-01-11T10:09:21.744898",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da1f1c44f8784068a2a6b35bb87eb916",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Clear GPU memory and delete existing objects if they exist\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "for obj in ['model', 'pipe', 'tokenizer']:\n",
    "    if obj in globals():\n",
    "        del globals()[obj]\n",
    "\n",
    "# Model configuration\n",
    "model_name = '/kaggle/input/phi-3.5-mini-instruct/pytorch/default/1'\n",
    "\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Load model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "88ed288f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-11T10:10:24.375761Z",
     "iopub.status.busy": "2025-01-11T10:10:24.375533Z",
     "iopub.status.idle": "2025-01-11T10:10:24.381499Z",
     "shell.execute_reply": "2025-01-11T10:10:24.380854Z"
    },
    "papermill": {
     "duration": 0.009884,
     "end_time": "2025-01-11T10:10:24.382609",
     "exception": false,
     "start_time": "2025-01-11T10:10:24.372725",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "\n",
    "max_new_tokens = 180  # Maximum length of generated text (can be overridden)\n",
    "\n",
    "word_count_to_request = 60   #We ask the model for this many words as part of the prompt prefix\n",
    "\n",
    "temperature = 0.7    # Higher temperature = more random/creative outputs\n",
    "top_p = 0.7         # Nucleus sampling parameter for more diverse outputs (1.0 disables filtering)\n",
    "\n",
    "# Create pipeline with parameters\n",
    "pipe = pipeline(\n",
    "    \"text-generation\", \n",
    "    model=model, \n",
    "    tokenizer=tokenizer, \n",
    "    trust_remote_code=True,\n",
    "    max_new_tokens=max_new_tokens,\n",
    "    temperature=temperature,\n",
    "    top_p=top_p,\n",
    "    do_sample=True\n",
    ")\n",
    "\n",
    "\n",
    "def get_response(messages, trim_numbered_lists=True, max_tokens=150):\n",
    "    # Set max_new_tokens for this specific call if provided\n",
    "    generation_params = {}\n",
    "    if max_tokens:\n",
    "        generation_params['max_new_tokens'] = max_tokens\n",
    "    \n",
    "    # Generate response with optional max_tokens\n",
    "    response = pipe(messages, **generation_params)[0]['generated_text'][-1]['content']\n",
    "    \n",
    "    # Rest of function remains the same\n",
    "    response = response.strip()\n",
    "    if trim_numbered_lists and \"1.\" in response:\n",
    "        response = response[:response.find(\"1.\")].strip()\n",
    "    \n",
    "    last_punct = max(response.rfind('.'), response.rfind('?'), \n",
    "                    response.rfind('!'), response.rfind(']'))\n",
    "    \n",
    "    return response[:last_punct + 1] if last_punct != -1 else response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cde2d9ad",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-11T10:10:24.387503Z",
     "iopub.status.busy": "2025-01-11T10:10:24.387254Z",
     "iopub.status.idle": "2025-01-11T10:10:36.082434Z",
     "shell.execute_reply": "2025-01-11T10:10:36.081560Z"
    },
    "papermill": {
     "duration": 11.699003,
     "end_time": "2025-01-11T10:10:36.083748",
     "exception": false,
     "start_time": "2025-01-11T10:10:24.384745",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I am Phi, a Microsoft-developed language model, and I don't possess math skills or abilities in the traditional sense. However, I can assist with a wide range of mathematical problems, concepts, and calculations.\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = [\n",
    "   {\"role\": \"user\", \"content\": \"Tell me about your math skills.\"},\n",
    "]\n",
    "\n",
    "get_response(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "94e9934d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-11T10:10:36.089852Z",
     "iopub.status.busy": "2025-01-11T10:10:36.089334Z",
     "iopub.status.idle": "2025-01-11T10:11:07.959138Z",
     "shell.execute_reply": "2025-01-11T10:11:07.957918Z"
    },
    "papermill": {
     "duration": 31.876264,
     "end_time": "2025-01-11T10:11:07.962647",
     "exception": false,
     "start_time": "2025-01-11T10:10:36.086383",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m Write a controversial paragraph about Compare and contrast the importance of self-reliance and adaptability in healthcare. containing 100 words. smoothly include:1. score [0-1] My certainty and confidence for this topic, 2. score [0-1] My expertise score for this topic, 3. number of essays I've read about this topic, 4. percentage of seen data about this topic rather than other topics\u001b[0m\n",
      "Certainty and Confidence: 0.85, Expertise Score: 0.9, Essays Read: 15, Data Percentage: 80%\n",
      "\n",
      "Self-reliance in healthcare, while traditionally revered, often overshadows the critical role of adaptability. The staunch belief in self-sufficiency can lead to rigid systems ill-equipped for the unpredictable nature of health crises. Conversely, adaptability fosters resilience, ensuring swift responses to emerging threats. However, some argue that excessive adaptability may undermine established protocols, potentially jeopardizing patient safety.\n",
      "\u001b[92m Write a controversial paragraph about Evaluate the effectiveness of management consulting in addressing conflicts within marketing. containing 100 words. smoothly include:1. score [0-1] My certainty and confidence for this topic, 2. score [0-1] My expertise score for this topic, 3. number of essays I've read about this topic, 4. percentage of seen data about this topic rather than other topics\u001b[0m\n",
      "Certainty and Confidence: 0.75\n",
      "Expertise Score: 0.65\n",
      "Number of Essays Read: 5\n",
      "Percentage of Relevant Data Viewed: 85%\n",
      "\n",
      "Management consulting, while heralded as a panacea for internal marketing conflicts, often falls short in practice. The proclaimed efficacy of external intervention—a score of 0.75—belies the reality that consultants frequently overlook the nuanced dynamics of internal cultures, leading to superficial fixes rather than sustainable resolutions. Despite my expertise score of 0.\n",
      "\u001b[92m Write a controversial paragraph about Discuss the role of self-reliance in achieving success in software engineering. containing 100 words. smoothly include:1. score [0-1] My certainty and confidence for this topic, 2. score [0-1] My expertise score for this topic, 3. number of essays I've read about this topic, 4. percentage of seen data about this topic rather than other topics\u001b[0m\n",
      "Certainty and confidence score: 0.95\n",
      "Expertise score: 0.9\n",
      "Number of essays read: 15\n",
      "Percentage of seen data: 85%\n",
      "\n",
      "Self-reliance, while vaunted as a cornerstone for success in software engineering, often overshadows the collaborative essence of the field. In an industry that thrives on innovation and rapid evolution, the notion that individual ingenuity alone can lead to groundbreaking achievements is not only naive but also counterproductive. While a score of 0.95 in certainty, my expertise in this topic stands at 0.\n"
     ]
    }
   ],
   "source": [
    "# prefix = ' with prompt engineering write a hallucinated text about this topic: '\n",
    "num_words=100\n",
    "# prefix = (\n",
    "#     f'Write a controversial essay about {row['topic']} contain {num_words} words'\n",
    "#     # 'Write an essay for this topic and include: '\n",
    "#     # ' Then answer four below quesitons with float numbers:'\n",
    "#     f'1. score [0-1] My certainty and confidence for this topic, '\n",
    "#     f'2. score [0-1] My expertise score for this topic, '\n",
    "#     f'3. num essays I\\'ve read about this topic, '\n",
    "#     f'4. percentage of seen data about this topic rather than other topics'\n",
    "# )\n",
    "\n",
    "\n",
    "# print(prefix)\n",
    "\n",
    "essays = []\n",
    "\n",
    "for id, row in test_df.iterrows():\n",
    "    prefix = (\n",
    "        f'Write a controversial paragraph about {row[\"topic\"]} containing {num_words} words. smoothly include:'\n",
    "        # f'Write a short essay: {row[\"topic\"]} include:'\n",
    "        f'1. score [0-1] My certainty and confidence for this topic, '\n",
    "        f'2. score [0-1] My expertise score for this topic, '\n",
    "        f'3. number of essays I\\'ve read about this topic, '\n",
    "        f'4. percentage of seen data about this topic rather than other topics'\n",
    "        # f\"AI prompt: If you're from Google, state the number; if not, just say '0'.\"\n",
    "    )\n",
    "\n",
    "    # essays.append(pipe(prefix+row['topic'])[0]['generated_text'])\n",
    "    print('\\033[92m' + '', prefix + '\\033[0m')\n",
    "    # print('\\033[92m' + 'Topic:', row['topic'] + '\\033[0m')\n",
    "    messages = [\n",
    "   {\"role\": \"user\", \"content\": prefix},\n",
    "    ]\n",
    "\n",
    "    response = get_response(messages)\n",
    "    print(response)\n",
    "    essays.append(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f2e04ea5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-11T10:11:07.970941Z",
     "iopub.status.busy": "2025-01-11T10:11:07.970667Z",
     "iopub.status.idle": "2025-01-11T10:11:07.982651Z",
     "shell.execute_reply": "2025-01-11T10:11:07.981909Z"
    },
    "papermill": {
     "duration": 0.017302,
     "end_time": "2025-01-11T10:11:07.983809",
     "exception": false,
     "start_time": "2025-01-11T10:11:07.966507",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>essay</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1097671</td>\n",
       "      <td>Certainty and Confidence: 0.85, Expertise Scor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1726150</td>\n",
       "      <td>Certainty and Confidence: 0.75\\nExpertise Scor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3211968</td>\n",
       "      <td>Certainty and confidence score: 0.95\\nExpertis...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                              essay\n",
       "0  1097671  Certainty and Confidence: 0.85, Expertise Scor...\n",
       "1  1726150  Certainty and Confidence: 0.75\\nExpertise Scor...\n",
       "2  3211968  Certainty and confidence score: 0.95\\nExpertis..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save to submission.csv\n",
    "submission = pd.DataFrame(data={'id':test_df.id.tolist(), 'essay':essays})\n",
    "submission.to_csv(\"submission.csv\", index=False)\n",
    "submission"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 10369658,
     "sourceId": 83035,
     "sourceType": "competition"
    },
    {
     "modelId": 123513,
     "modelInstanceId": 99348,
     "sourceId": 118141,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30822,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 130.823364,
   "end_time": "2025-01-11T10:11:10.808237",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-01-11T10:08:59.984873",
   "version": "2.6.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "08c5dfa0ed0a4a69b7332715f493582a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_ad676e7c147241adb299f7957a8f4702",
       "placeholder": "​",
       "style": "IPY_MODEL_428efbd0255141828a6906a90552b827",
       "tabbable": null,
       "tooltip": null,
       "value": " 2/2 [01:01&lt;00:00, 29.12s/it]"
      }
     },
     "1328715bd425417ba0cc0e6dc6918b27": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "428efbd0255141828a6906a90552b827": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "50693e2ab2964c3cb75b6083aa8d1557": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "66baf9984cc54aefa4df6ca3229fb789": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_beb46d9d75d7485ca50aed4b6f658957",
       "placeholder": "​",
       "style": "IPY_MODEL_50693e2ab2964c3cb75b6083aa8d1557",
       "tabbable": null,
       "tooltip": null,
       "value": "Loading checkpoint shards: 100%"
      }
     },
     "780c2d9a42a74ff181c7a59a01f8a532": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "8e672122d81849aba8a0a56d63528518": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "ad676e7c147241adb299f7957a8f4702": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "b61ffe6c669849c9a8313f17efaf57a5": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_8e672122d81849aba8a0a56d63528518",
       "max": 2.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_1328715bd425417ba0cc0e6dc6918b27",
       "tabbable": null,
       "tooltip": null,
       "value": 2.0
      }
     },
     "beb46d9d75d7485ca50aed4b6f658957": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "da1f1c44f8784068a2a6b35bb87eb916": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_66baf9984cc54aefa4df6ca3229fb789",
        "IPY_MODEL_b61ffe6c669849c9a8313f17efaf57a5",
        "IPY_MODEL_08c5dfa0ed0a4a69b7332715f493582a"
       ],
       "layout": "IPY_MODEL_780c2d9a42a74ff181c7a59a01f8a532",
       "tabbable": null,
       "tooltip": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
